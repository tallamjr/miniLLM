# üóúÔ∏è `miniLLM`

Miniature LLMs deployed to edge devices. Inspired by the NeurIPS '24 [Edge-Device Large Language Model Competition](https://edge-llms-challenge.github.io/edge-llm-challenge.github.io/index)

## Awesome LLM Compression

- [NeuralMagic's LLM Compressor](https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/)
  - https://github.com/vllm-project/llm-compressor
- [int4 Weight Quantization](https://github.com/vllm-project/llm-compressor/tree/main/examples/quantization_w4a16)
- [Quantize ONNX Models](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)
  - https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/registry.py
  - https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb

### Papers

- [LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment](https://arxiv.org/pdf/2410.21352)
